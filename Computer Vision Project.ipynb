{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2af0e35c",
   "metadata": {},
   "source": [
    "# Botanical Research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7360feef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow\n",
    "tensorflow.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6e3419b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers, optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7cc9dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the random number generator\n",
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "# Ignore the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c25bb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_path = r\"C:\\Users\\ac253\\AINML\\Computer Vision Week 3 (Project 1)\\train-20210824T073124Z-001.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed7f5552",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "with ZipFile(images_path, 'r') as zip:\n",
    "  zip.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "900fe1bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', 'Computer vision project.ipynb', 'CV 1 - Project-1.pdf', 'Predict.png', 'Seedling - Prediction-20210824T073124Z-001.zip', 'train', 'train-20210824T073124Z-001.zip']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "cwd=os.getcwd()\n",
    "print(os.listdir(cwd))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e3e73f",
   "metadata": {},
   "source": [
    "We can see that a folder called train has been generated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49031f37",
   "metadata": {},
   "source": [
    "## Training a Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbc52f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(lr,lamb,verb=True):\n",
    "    learning_rate=1e-3\n",
    "    lamb = 1e-5\n",
    "    hidden_nodes=324\n",
    "    output_nodes=12\n",
    "    \n",
    "    model=Sequential()\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(hidden_nodes, input_shape=(1024,),activation='relu'))\n",
    "    model.add(Dense(hidden_nodes,activation='relu'))\n",
    "    model.add(Dense(output_nodes,activation='softmax',kernel_regularizer=regularizers.l2(lamb)))\n",
    "    \n",
    "    sgd=optimizers.SGD(lr=learning_rate,decay=1e-6, momentum=0.9)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "    \n",
    "    model.fit_generator(\n",
    "    train_set,\n",
    "    steps_per_epoch = train_generator.samples // 32,\n",
    "    validation_data = validation_set, \n",
    "    validation_steps = validation_generator.samples // 32,\n",
    "    epochs = 20)\n",
    "    score=model.evaluate(train_generator,verbose=0)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b779ce5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3816 images belonging to 12 classes.\n",
      "Found 951 images belonging to 12 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_data_dir=r\"C:\\Users\\ac253\\AINML\\Computer Vision Week 3 (Project 1)\\train\"\n",
    "\n",
    "# Create data generator for training data with data augmentation and normalizing all\n",
    "# values by 255\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2) # set validation split\n",
    "\n",
    "# Setting training data generator's source directory\n",
    "# Setting the target size to resize all the images to (64,64) as the model input layer expects 32X32 images\n",
    "\n",
    "train_set = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(16, 16),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='training') # set as training data\n",
    "\n",
    "validation_set = train_datagen.flow_from_directory(\n",
    "    train_data_dir, # same directory as training data\n",
    "    target_size=(16, 16),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='validation') # set as validation data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e34de1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "119/119 [==============================] - 41s 340ms/step - loss: 2.4196 - accuracy: 0.1535 - val_loss: 2.3995 - val_accuracy: 0.1412\n",
      "Epoch 2/20\n",
      "119/119 [==============================] - 38s 321ms/step - loss: 2.3881 - accuracy: 0.1797 - val_loss: 2.3824 - val_accuracy: 0.1552\n",
      "Epoch 3/20\n",
      "119/119 [==============================] - 39s 324ms/step - loss: 2.3741 - accuracy: 0.1945 - val_loss: 2.3631 - val_accuracy: 0.2177\n",
      "Epoch 4/20\n",
      "119/119 [==============================] - 39s 330ms/step - loss: 2.3537 - accuracy: 0.2080 - val_loss: 2.3503 - val_accuracy: 0.1649\n",
      "Epoch 5/20\n",
      "119/119 [==============================] - 39s 324ms/step - loss: 2.3341 - accuracy: 0.2146 - val_loss: 2.3229 - val_accuracy: 0.2274\n",
      "Epoch 6/20\n",
      "119/119 [==============================] - 39s 327ms/step - loss: 2.3122 - accuracy: 0.2289 - val_loss: 2.3057 - val_accuracy: 0.2338\n",
      "Epoch 7/20\n",
      "119/119 [==============================] - 38s 325ms/step - loss: 2.2891 - accuracy: 0.2368 - val_loss: 2.2838 - val_accuracy: 0.2252\n",
      "Epoch 8/20\n",
      "119/119 [==============================] - 39s 328ms/step - loss: 2.2630 - accuracy: 0.2376 - val_loss: 2.2583 - val_accuracy: 0.2155\n",
      "Epoch 9/20\n",
      "119/119 [==============================] - 39s 326ms/step - loss: 2.2360 - accuracy: 0.2487 - val_loss: 2.2269 - val_accuracy: 0.2575\n",
      "Epoch 10/20\n",
      "119/119 [==============================] - 39s 325ms/step - loss: 2.2063 - accuracy: 0.2571 - val_loss: 2.2033 - val_accuracy: 0.2478\n",
      "Epoch 11/20\n",
      "119/119 [==============================] - 39s 323ms/step - loss: 2.1756 - accuracy: 0.2659 - val_loss: 2.1679 - val_accuracy: 0.2651\n",
      "Epoch 12/20\n",
      "119/119 [==============================] - 39s 324ms/step - loss: 2.1521 - accuracy: 0.2762 - val_loss: 2.1412 - val_accuracy: 0.2662\n",
      "Epoch 13/20\n",
      "119/119 [==============================] - 38s 322ms/step - loss: 2.1240 - accuracy: 0.2680 - val_loss: 2.1101 - val_accuracy: 0.2640\n",
      "Epoch 14/20\n",
      "119/119 [==============================] - 38s 316ms/step - loss: 2.0924 - accuracy: 0.2899 - val_loss: 2.0909 - val_accuracy: 0.2759\n",
      "Epoch 15/20\n",
      "119/119 [==============================] - 39s 332ms/step - loss: 2.0636 - accuracy: 0.2902 - val_loss: 2.0684 - val_accuracy: 0.2780\n",
      "Epoch 16/20\n",
      "119/119 [==============================] - 39s 329ms/step - loss: 2.0358 - accuracy: 0.3039 - val_loss: 2.0382 - val_accuracy: 0.2726\n",
      "Epoch 17/20\n",
      "119/119 [==============================] - 39s 331ms/step - loss: 2.0069 - accuracy: 0.3142 - val_loss: 2.0004 - val_accuracy: 0.2963\n",
      "Epoch 18/20\n",
      "119/119 [==============================] - 39s 331ms/step - loss: 1.9701 - accuracy: 0.3161 - val_loss: 1.9807 - val_accuracy: 0.3050\n",
      "Epoch 19/20\n",
      "119/119 [==============================] - 39s 327ms/step - loss: 1.9501 - accuracy: 0.3145 - val_loss: 1.9389 - val_accuracy: 0.3373\n",
      "Epoch 20/20\n",
      "119/119 [==============================] - 39s 327ms/step - loss: 1.9251 - accuracy: 0.3251 - val_loss: 1.9075 - val_accuracy: 0.3103\n"
     ]
    }
   ],
   "source": [
    "learning_rate=1e-3\n",
    "lamb = 1e-5\n",
    "hidden_nodes=324\n",
    "output_nodes=12\n",
    "    \n",
    "model=Sequential()\n",
    "model.add(Flatten())\n",
    "model.add(Dense(hidden_nodes, input_shape=(768,),activation='relu'))\n",
    "model.add(Dense(hidden_nodes,activation='relu'))\n",
    "model.add(Dense(output_nodes,activation='softmax',kernel_regularizer=regularizers.l2(lamb)))\n",
    "\n",
    "sgd=optimizers.SGD(lr=learning_rate,decay=1e-6, momentum=0.9)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "model.fit_generator(\n",
    "train_set,\n",
    "steps_per_epoch = train_set.samples // 32,\n",
    "validation_data = validation_set, \n",
    "validation_steps = validation_set.samples // 32,\n",
    "epochs = 20)\n",
    "\n",
    "score=model.evaluate(train_set,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b970980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 16, 3)\n",
      "After expand_dims: (1, 16, 16, 3)\n",
      "{'Black-grass': 0, 'Charlock': 1, 'Cleavers': 2, 'Common Chickweed': 3, 'Common wheat': 4, 'Fat Hen': 5, 'Loose Silky-bent': 6, 'Maize': 7, 'Scentless Mayweed': 8, 'Shepherds Purse': 9, 'Small-flowered Cranesbill': 10, 'Sugar beet': 11}\n",
      "[0.07028779 0.53341454 0.03721414 0.02631604 0.04447062 0.04294013\n",
      " 0.00817322 0.02281118 0.03006822 0.014699   0.12303982 0.04656532]\n",
      "Charlock\n"
     ]
    }
   ],
   "source": [
    "test_image1 = cv2.imread(r'C:\\Users\\ac253\\AINML\\Computer Vision Week 3 (Project 1)\\Predict.png')\n",
    "# Resize the image to 64X64 shape to be compatible with the model\n",
    "test_image1 = cv2.resize(test_image1,(16,16))\n",
    "\n",
    "# Check if the size of the Image array is compatible with Keras model\n",
    "print(test_image1.shape)\n",
    "\n",
    "# If not compatible expand the dimensions to match with the Keras Input\n",
    "test_image1 = np.expand_dims(test_image1, axis = 0)\n",
    "test_image1 =test_image1*1/255.0\n",
    "\n",
    "#Check the size of the Image array again\n",
    "print('After expand_dims: '+ str(test_image1.shape))\n",
    "\n",
    "\n",
    "#Predict the result of the test image\n",
    "result = model.predict(test_image1)\n",
    "\n",
    "# Check the indices Image Data Generator has allotted to each folder\n",
    "classes_dict = train_set.class_indices\n",
    "print(classes_dict)\n",
    "\n",
    "# Creating a list of classes in test set for showing the result as the folder name\n",
    "prediction_class = []\n",
    "for class_name,index in classes_dict.items():\n",
    "  prediction_class.append(class_name)\n",
    "  \n",
    "print(result[0])\n",
    "\n",
    "# Index of the class with maximum probability\n",
    "predicted_index = np.argmax(result[0])\n",
    "\n",
    "# Print the name of the class\n",
    "print(prediction_class[predicted_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaae14b0",
   "metadata": {},
   "source": [
    "## Training a CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9bc9e728",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building the CNN clasifier.\n",
    "classifier=Sequential()\n",
    "\n",
    "# Adding a conv layer with 32 kernels of 3x3 shape and activation function Relu\n",
    "classifier.add(Conv2D(32,(3,3), input_shape =(64,64,3),activation ='relu', padding = 'same'))\n",
    "\n",
    "# Adding a maxpooling layer of size 2x2\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "# Add another Convolution layer with 32 kernels of 3X3 shape with activation function ReLU\n",
    "classifier.add(Conv2D(32, (3, 3), activation = 'relu', padding = 'same'))\n",
    "\n",
    "# Adding a maxpooling layer of size 2x2\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "# Add another Convolution layer with 32 kernels of 3X3 shape with activation function ReLU\n",
    "classifier.add(Conv2D(32, (3, 3), activation = 'relu', padding = 'same'))\n",
    "\n",
    "# Adding a maxpooling layer of size 2x2\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "# Flattening the layers before the fully connected layer\n",
    "classifier.add(Flatten())\n",
    "\n",
    "# Adding fully connected layer with 512 neurons\n",
    "classifier.add(Dense(units=512, activation='relu'))\n",
    "\n",
    "# Adding a dropout with the probability of 0.5\n",
    "classifier.add(Dropout(0.5))\n",
    "\n",
    "# Adding a fully connected layer with 128 neurons\n",
    "classifier.add(Dense(units=128,activation='relu'))\n",
    "\n",
    "# The final output layer with 12 neurons to predict the categorical classification\n",
    "classifier.add(Dense(units=12,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b1d18ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 64, 64, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 16, 16, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               1049088   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 12)                1548      \n",
      "=================================================================\n",
      "Total params: 1,135,692\n",
      "Trainable params: 1,135,692\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Compiling the CNN classifier with Adam optimizer (default Learning rate and other parameters) and Categorical Crossentropy as loss function and Accuracy as the metric to monitor\n",
    "\n",
    "opt= Adam(lr=0.001, beta_1=0.9, beta_2=0.999, decay=0.001, epsilon=None,amsgrad=False)\n",
    "classifier.compile(optimizer=opt,loss= 'categorical_crossentropy', metrics=['accuracy'])\n",
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "215976ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Pre-processing\n",
    "# ImageDataGenerator is a powerful preprocessing utility to generate training and testing data with common data augmentation techniques. It can also be used to generate training data from Images stored in hierarchical directory structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2dd3c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3816 images belonging to 12 classes.\n",
      "Found 951 images belonging to 12 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_data_dir=r\"C:\\Users\\ac253\\AINML\\Computer Vision Week 3 (Project 1)\\train\"\n",
    "\n",
    "# Create data generator for training data with data augmentation and normalizing all\n",
    "# values by 255\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2) # set validation split\n",
    "\n",
    "# Setting training data generator's source directory\n",
    "# Setting the target size to resize all the images to (64,64) as the model input layer expects 32X32 images\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(64, 64),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='training') # set as training data\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir, # same directory as training data\n",
    "    target_size=(64, 64),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='validation') # set as validation data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ea93e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "119/119 [==============================] - 47s 362ms/step - loss: 2.4263 - accuracy: 0.1401 - val_loss: 2.3935 - val_accuracy: 0.2284\n",
      "Epoch 2/20\n",
      "119/119 [==============================] - 43s 359ms/step - loss: 2.0226 - accuracy: 0.3052 - val_loss: 1.6358 - val_accuracy: 0.4181\n",
      "Epoch 3/20\n",
      "119/119 [==============================] - 46s 384ms/step - loss: 1.6354 - accuracy: 0.4057 - val_loss: 1.4463 - val_accuracy: 0.4925\n",
      "Epoch 4/20\n",
      "119/119 [==============================] - 43s 359ms/step - loss: 1.4082 - accuracy: 0.5066 - val_loss: 1.3307 - val_accuracy: 0.5302\n",
      "Epoch 5/20\n",
      "119/119 [==============================] - 43s 359ms/step - loss: 1.2019 - accuracy: 0.5748 - val_loss: 1.1181 - val_accuracy: 0.6369\n",
      "Epoch 6/20\n",
      "119/119 [==============================] - 43s 363ms/step - loss: 1.0990 - accuracy: 0.6123 - val_loss: 1.0346 - val_accuracy: 0.6509\n",
      "Epoch 7/20\n",
      "119/119 [==============================] - 42s 357ms/step - loss: 1.0121 - accuracy: 0.6409 - val_loss: 0.9672 - val_accuracy: 0.6649\n",
      "Epoch 8/20\n",
      "119/119 [==============================] - 42s 357ms/step - loss: 0.9129 - accuracy: 0.6792 - val_loss: 0.8660 - val_accuracy: 0.7155\n",
      "Epoch 9/20\n",
      "119/119 [==============================] - 43s 359ms/step - loss: 0.8669 - accuracy: 0.6940 - val_loss: 0.7983 - val_accuracy: 0.7446\n",
      "Epoch 10/20\n",
      "119/119 [==============================] - 42s 357ms/step - loss: 0.8057 - accuracy: 0.7162 - val_loss: 0.7519 - val_accuracy: 0.7662\n",
      "Epoch 11/20\n",
      "119/119 [==============================] - 42s 356ms/step - loss: 0.7490 - accuracy: 0.7386 - val_loss: 0.7499 - val_accuracy: 0.7651\n",
      "Epoch 12/20\n",
      "119/119 [==============================] - 42s 357ms/step - loss: 0.7161 - accuracy: 0.7521 - val_loss: 0.7631 - val_accuracy: 0.7543\n",
      "Epoch 13/20\n",
      "119/119 [==============================] - 42s 354ms/step - loss: 0.6762 - accuracy: 0.7637 - val_loss: 0.7245 - val_accuracy: 0.7726\n",
      "Epoch 14/20\n",
      "119/119 [==============================] - 42s 355ms/step - loss: 0.6515 - accuracy: 0.7725 - val_loss: 0.6925 - val_accuracy: 0.7769\n",
      "Epoch 15/20\n",
      "119/119 [==============================] - 42s 357ms/step - loss: 0.6535 - accuracy: 0.7703 - val_loss: 0.6379 - val_accuracy: 0.8190\n",
      "Epoch 16/20\n",
      "119/119 [==============================] - 43s 362ms/step - loss: 0.6272 - accuracy: 0.7867 - val_loss: 0.6531 - val_accuracy: 0.7942\n",
      "Epoch 17/20\n",
      "119/119 [==============================] - 42s 354ms/step - loss: 0.6002 - accuracy: 0.7814 - val_loss: 0.6442 - val_accuracy: 0.7996\n",
      "Epoch 18/20\n",
      "119/119 [==============================] - 41s 343ms/step - loss: 0.5676 - accuracy: 0.8002 - val_loss: 0.6572 - val_accuracy: 0.7985\n",
      "Epoch 19/20\n",
      "119/119 [==============================] - 41s 341ms/step - loss: 0.5474 - accuracy: 0.8005 - val_loss: 0.5765 - val_accuracy: 0.8308\n",
      "Epoch 20/20\n",
      "119/119 [==============================] - 40s 339ms/step - loss: 0.5374 - accuracy: 0.8055 - val_loss: 0.5961 - val_accuracy: 0.8179\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x156cbcba280>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch = train_generator.samples // 32,\n",
    "    validation_data = validation_generator, \n",
    "    validation_steps = validation_generator.samples // 32,\n",
    "    epochs = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66a00e9",
   "metadata": {},
   "source": [
    "### In the 20th epoch we have managed to obtain a training accuracy of 0.8055 and validation accuracy of 0.8179"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a728a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model and its weights after training\n",
    "classifier.save('./classifier.h5')\n",
    "\n",
    "classifier.save_weights('./classifier_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "488f72e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', 'classifier.h5', 'classifier_weights.h5', 'Computer vision project.ipynb', 'CV 1 - Project-1.pdf', 'Predict.png', 'Seedling - Prediction-20210824T073124Z-001.zip', 'train', 'train-20210824T073124Z-001.zip']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(cwd))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8877b2f",
   "metadata": {},
   "source": [
    "We can see that the classifier and its weights are saved in current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e6551d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test the model now\n",
    "# Loading the pre-trained saved model\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "# Load the pre trained model from the HDF5 file saved previously\n",
    "pretrained_model = load_model('./classifier.h5')\n",
    "pretrained_model.load_weights('./classifier_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6764ffbd",
   "metadata": {},
   "source": [
    "Testing the model on a test image from one of the test folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "78678e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 64, 3)\n",
      "After expand_dims: (1, 64, 64, 3)\n",
      "{'Black-grass': 0, 'Charlock': 1, 'Cleavers': 2, 'Common Chickweed': 3, 'Common wheat': 4, 'Fat Hen': 5, 'Loose Silky-bent': 6, 'Maize': 7, 'Scentless Mayweed': 8, 'Shepherds Purse': 9, 'Small-flowered Cranesbill': 10, 'Sugar beet': 11}\n",
      "[5.19503196e-08 9.21515405e-01 3.57701890e-02 4.07363956e-08\n",
      " 1.37994948e-06 3.25634405e-02 2.20534417e-08 8.47028848e-03\n",
      " 2.68357748e-04 1.15911644e-04 1.26988476e-03 2.52227310e-05]\n",
      "Charlock\n"
     ]
    }
   ],
   "source": [
    "test_image = cv2.imread(r'C:\\Users\\ac253\\AINML\\Computer Vision Week 3 (Project 1)\\Predict.png')\n",
    "# Resize the image to 64X64 shape to be compatible with the model\n",
    "test_image = cv2.resize(test_image,(64,64))\n",
    "\n",
    "# Check if the size of the Image array is compatible with Keras model\n",
    "print(test_image.shape)\n",
    "\n",
    "# If not compatible expand the dimensions to match with the Keras Input\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "test_image =test_image*1/255.0\n",
    "\n",
    "#Check the size of the Image array again\n",
    "print('After expand_dims: '+ str(test_image.shape))\n",
    "\n",
    "\n",
    "#Predict the result of the test image\n",
    "result = pretrained_model.predict(test_image)\n",
    "\n",
    "# Check the indices Image Data Generator has allotted to each folder\n",
    "classes_dict = train_generator.class_indices\n",
    "print(classes_dict)\n",
    "\n",
    "# Creating a list of classes in test set for showing the result as the folder name\n",
    "prediction_class = []\n",
    "for class_name,index in classes_dict.items():\n",
    "  prediction_class.append(class_name)\n",
    "  \n",
    "print(result[0])\n",
    "\n",
    "# Index of the class with maximum probability\n",
    "predicted_index = np.argmax(result[0])\n",
    "\n",
    "# Print the name of the class\n",
    "print(prediction_class[predicted_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae85350",
   "metadata": {},
   "source": [
    "### Comparing the results from above algorithms we can say that the CNN has better training accuracy and validation accuracy than the Neural Network. \n",
    "In CNN we used 64x64 images to train the classifier. Where as we used only 16x16 images to train the Neural Network. Using higher dimension input images would have created a lot of trainable parameters for such a huge dataset. Thus there would have a lack of memory to train such huge set of parameters or it would have take a lot time to train over my RTX 2060. Since neural networks are very hardware dependent.\n",
    "CNN on the other hand does not have a large number of trainable parameters compared to neural networks. CNNs are also robust when it comes to image recognition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7c6108",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
